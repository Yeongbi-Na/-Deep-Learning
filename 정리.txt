element-wise operations: 각 값에 동일하게 operation 적용
	ex) relu, 더하기
	
naive-relu : 전체 행렬 다 돌아다니면서 relu 함수를 적용
x.ndim 이나 x.shape 동일한 값 [1,2,3] 이면 3 반환/ [[1],[2]]면 2,1 반환
>> 하나씩 값을 돌아다니면서 연산을 수행하는 element-wise 연산은 비효율적 >> BLAS or x+y or np.maximum(z,0) 으로 구현 가능




Broadcasting
: shape이 다르면 어떻게 연산 수행? element-wise로 할 수 없음
모호성이 없으면 작은 tensor를 큰 tensor에 맞춰 뻥튀기!! "브로드캐스팅"
step1: 작은 텐서에 축을 추가
step2: 큰 텐서와 전체의 쉐입과 동일해지도록 브로드 축에 반복적으로 값을 추가하며 뻥튀기

ex)shape이  x [32,10] y [10,]
step1: y> (1,10)
step2: 이 y 를 32번 반복 
>>컴퓨터에서는 이런 과정이 가상화되어 구현됨

ex) shape x(64,3,32,10) y(10,)
step1: y(1,1,1,10)
step2: 64,3,32만큼 반복
>>그냥 np.maximum(x,y)하면 내부적으로 자동으로 위 과정을 수행해서 값을 반환해줌





dot product >> element-wise multiplecation과는 다름
e-w product는 *를 이용해서 수행/ 배열에 있는 값들을 element-wise로 곱하는 것
dot product는 매트릭스 곱임.

두개의 tensor 중 하나의 ndim이 1보다 크면 dot(x,y)!=dot(y,x) 
dot(x,y)의 결과 shape은 (x.shape[0],y.shape[1]) 임




reshape!






















